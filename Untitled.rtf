{\rtf1\ansi\ansicpg1252\cocoartf2578
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red202\green202\blue202;\red23\green23\blue23;\red212\green212\blue212;
\red167\green197\blue152;}
{\*\expandedcolortbl;;\cssrgb\c83137\c83137\c83137;\cssrgb\c11765\c11765\c11765;\cssrgb\c86275\c86275\c86275;
\cssrgb\c70980\c80784\c65882;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl360\partightenfactor0

\f0\fs24 \cf2 \cb3 \expnd0\expndtw0\kerning0
Training Loss \cf4 (\cf2 after epoch \cf5 0\cf4 ):\cf2  tensor\cf4 (\cf5 188.4420\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 1\cf4 ):\cf2  tensor\cf4 (\cf5 137.2105\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 2\cf4 ):\cf2  tensor\cf4 (\cf5 117.5774\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 3\cf4 ):\cf2  tensor\cf4 (\cf5 103.4433\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 4\cf4 ):\cf2  tensor\cf4 (\cf5 93.1011\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 5\cf4 ):\cf2  tensor\cf4 (\cf5 82.9767\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 6\cf4 ):\cf2  tensor\cf4 (\cf5 75.3768\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 7\cf4 ):\cf2  tensor\cf4 (\cf5 67.3586\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 8\cf4 ):\cf2  tensor\cf4 (\cf5 60.5762\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 9\cf4 ):\cf2  tensor\cf4 (\cf5 52.8925\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 10\cf4 ):\cf2  tensor\cf4 (\cf5 47.2274\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 11\cf4 ):\cf2  tensor\cf4 (\cf5 42.5479\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 12\cf4 ):\cf2  tensor\cf4 (\cf5 38.9599\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 13\cf4 ):\cf2  tensor\cf4 (\cf5 33.7130\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 14\cf4 ):\cf2  tensor\cf4 (\cf5 29.5782\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 15\cf4 ):\cf2  tensor\cf4 (\cf5 28.3131\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 16\cf4 ):\cf2  tensor\cf4 (\cf5 23.5500\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 17\cf4 ):\cf2  tensor\cf4 (\cf5 20.0140\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 18\cf4 ):\cf2  tensor\cf4 (\cf5 18.7563\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 19\cf4 ):\cf2  tensor\cf4 (\cf5 17.7299\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 20\cf4 ):\cf2  tensor\cf4 (\cf5 14.1713\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 21\cf4 ):\cf2  tensor\cf4 (\cf5 12.9637\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 22\cf4 ):\cf2  tensor\cf4 (\cf5 10.7153\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 23\cf4 ):\cf2  tensor\cf4 (\cf5 10.1071\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 24\cf4 ):\cf2  tensor\cf4 (\cf5 10.5659\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 25\cf4 ):\cf2  tensor\cf4 (\cf5 7.3418\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 26\cf4 ):\cf2  tensor\cf4 (\cf5 6.3983\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 27\cf4 ):\cf2  tensor\cf4 (\cf5 7.1001\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 28\cf4 ):\cf2  tensor\cf4 (\cf5 17.8485\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 29\cf4 ):\cf2  tensor\cf4 (\cf5 6.4342\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 30\cf4 ):\cf2  tensor\cf4 (\cf5 3.6081\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 31\cf4 ):\cf2  tensor\cf4 (\cf5 3.4122\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 32\cf4 ):\cf2  tensor\cf4 (\cf5 3.9302\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 33\cf4 ):\cf2  tensor\cf4 (\cf5 13.5016\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 34\cf4 ):\cf2  tensor\cf4 (\cf5 3.9744\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 35\cf4 ):\cf2  tensor\cf4 (\cf5 2.0438\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 36\cf4 ):\cf2  tensor\cf4 (\cf5 1.7443\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 37\cf4 ):\cf2  tensor\cf4 (\cf5 1.4405\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 38\cf4 ):\cf2  tensor\cf4 (\cf5 1.2770\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 39\cf4 ):\cf2  tensor\cf4 (\cf5 7.1046\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 40\cf4 ):\cf2  tensor\cf4 (\cf5 26.3392\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 0\cf4 ):\cf2  tensor\cf4 (\cf5 192.4917\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 1\cf4 ):\cf2  tensor\cf4 (\cf5 136.2292\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 2\cf4 ):\cf2  tensor\cf4 (\cf5 113.2490\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 3\cf4 ):\cf2  tensor\cf4 (\cf5 102.0329\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 4\cf4 ):\cf2  tensor\cf4 (\cf5 87.8279\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 5\cf4 ):\cf2  tensor\cf4 (\cf5 80.1776\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 6\cf4 ):\cf2  tensor\cf4 (\cf5 71.4162\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 7\cf4 ):\cf2  tensor\cf4 (\cf5 64.3933\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 8\cf4 ):\cf2  tensor\cf4 (\cf5 58.7755\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 9\cf4 ):\cf2  tensor\cf4 (\cf5 52.0315\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 10\cf4 ):\cf2  tensor\cf4 (\cf5 48.2122\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 11\cf4 ):\cf2  tensor\cf4 (\cf5 42.5670\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 12\cf4 ):\cf2  tensor\cf4 (\cf5 38.3003\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 13\cf4 ):\cf2  tensor\cf4 (\cf5 34.9354\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 14\cf4 ):\cf2  tensor\cf4 (\cf5 31.3329\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 15\cf4 ):\cf2  tensor\cf4 (\cf5 27.0687\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 16\cf4 ):\cf2  tensor\cf4 (\cf5 22.4773\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 17\cf4 ):\cf2  tensor\cf4 (\cf5 20.7820\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 18\cf4 ):\cf2  tensor\cf4 (\cf5 20.0568\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 19\cf4 ):\cf2  tensor\cf4 (\cf5 17.3081\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 20\cf4 ):\cf2  tensor\cf4 (\cf5 13.9286\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 21\cf4 ):\cf2  tensor\cf4 (\cf5 13.6342\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 22\cf4 ):\cf2  tensor\cf4 (\cf5 10.5435\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 23\cf4 ):\cf2  tensor\cf4 (\cf5 12.5347\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 24\cf4 ):\cf2  tensor\cf4 (\cf5 10.6417\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 25\cf4 ):\cf2  tensor\cf4 (\cf5 7.7843\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 26\cf4 ):\cf2  tensor\cf4 (\cf5 11.8257\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 27\cf4 ):\cf2  tensor\cf4 (\cf5 6.3449\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 28\cf4 ):\cf2  tensor\cf4 (\cf5 4.9892\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 29\cf4 ):\cf2  tensor\cf4 (\cf5 6.6010\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 30\cf4 ):\cf2  tensor\cf4 (\cf5 10.2208\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 31\cf4 ):\cf2  tensor\cf4 (\cf5 5.2878\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 32\cf4 ):\cf2  tensor\cf4 (\cf5 3.5139\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 33\cf4 ):\cf2  tensor\cf4 (\cf5 2.3546\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 34\cf4 ):\cf2  tensor\cf4 (\cf5 3.3464\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 35\cf4 ):\cf2  tensor\cf4 (\cf5 8.7264\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 36\cf4 ):\cf2  tensor\cf4 (\cf5 10.2655\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 37\cf4 ):\cf2  tensor\cf4 (\cf5 3.6600\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 38\cf4 ):\cf2  tensor\cf4 (\cf5 2.0487\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 39\cf4 ):\cf2  tensor\cf4 (\cf5 1.1567\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 40\cf4 ):\cf2  tensor\cf4 (\cf5 17.8944\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 0\cf4 ):\cf2  tensor\cf4 (\cf5 189.5135\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 1\cf4 ):\cf2  tensor\cf4 (\cf5 135.0748\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 2\cf4 ):\cf2  tensor\cf4 (\cf5 112.8221\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 3\cf4 ):\cf2  tensor\cf4 (\cf5 99.5670\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 4\cf4 ):\cf2  tensor\cf4 (\cf5 88.0953\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 5\cf4 ):\cf2  tensor\cf4 (\cf5 78.4426\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 6\cf4 ):\cf2  tensor\cf4 (\cf5 68.9199\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 7\cf4 ):\cf2  tensor\cf4 (\cf5 62.3487\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 8\cf4 ):\cf2  tensor\cf4 (\cf5 55.5289\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 9\cf4 ):\cf2  tensor\cf4 (\cf5 50.3270\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 10\cf4 ):\cf2  tensor\cf4 (\cf5 44.7176\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 11\cf4 ):\cf2  tensor\cf4 (\cf5 40.1162\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 12\cf4 ):\cf2  tensor\cf4 (\cf5 34.3587\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 13\cf4 ):\cf2  tensor\cf4 (\cf5 34.3470\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 14\cf4 ):\cf2  tensor\cf4 (\cf5 27.1048\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 15\cf4 ):\cf2  tensor\cf4 (\cf5 24.2522\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 16\cf4 ):\cf2  tensor\cf4 (\cf5 20.9690\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 17\cf4 ):\cf2  tensor\cf4 (\cf5 19.1953\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 18\cf4 ):\cf2  tensor\cf4 (\cf5 17.7631\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 19\cf4 ):\cf2  tensor\cf4 (\cf5 15.5051\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 20\cf4 ):\cf2  tensor\cf4 (\cf5 12.2313\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 21\cf4 ):\cf2  tensor\cf4 (\cf5 10.0893\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 22\cf4 ):\cf2  tensor\cf4 (\cf5 8.7647\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 23\cf4 ):\cf2  tensor\cf4 (\cf5 8.0605\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 24\cf4 ):\cf2  tensor\cf4 (\cf5 8.4003\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 25\cf4 ):\cf2  tensor\cf4 (\cf5 7.1792\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 26\cf4 ):\cf2  tensor\cf4 (\cf5 8.3164\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 27\cf4 ):\cf2  tensor\cf4 (\cf5 5.3986\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 28\cf4 ):\cf2  tensor\cf4 (\cf5 5.4919\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 29\cf4 ):\cf2  tensor\cf4 (\cf5 11.3910\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 30\cf4 ):\cf2  tensor\cf4 (\cf5 7.9808\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 31\cf4 ):\cf2  tensor\cf4 (\cf5 2.5654\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 32\cf4 ):\cf2  tensor\cf4 (\cf5 2.0357\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 33\cf4 ):\cf2  tensor\cf4 (\cf5 2.3216\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 34\cf4 ):\cf2  tensor\cf4 (\cf5 1.5312\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 35\cf4 ):\cf2  tensor\cf4 (\cf5 1.4201\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 36\cf4 ):\cf2  tensor\cf4 (\cf5 18.8310\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 37\cf4 ):\cf2  tensor\cf4 (\cf5 12.1674\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 38\cf4 ):\cf2  tensor\cf4 (\cf5 2.7597\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 39\cf4 ):\cf2  tensor\cf4 (\cf5 1.3629\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 40\cf4 ):\cf2  tensor\cf4 (\cf5 1.0441\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 0\cf4 ):\cf2  tensor\cf4 (\cf5 193.2372\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 1\cf4 ):\cf2  tensor\cf4 (\cf5 138.2170\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 2\cf4 ):\cf2  tensor\cf4 (\cf5 115.4308\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 3\cf4 ):\cf2  tensor\cf4 (\cf5 100.0831\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 4\cf4 ):\cf2  tensor\cf4 (\cf5 89.3211\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 5\cf4 ):\cf2  tensor\cf4 (\cf5 79.8358\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 6\cf4 ):\cf2  tensor\cf4 (\cf5 72.3653\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 7\cf4 ):\cf2  tensor\cf4 (\cf5 64.7811\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 8\cf4 ):\cf2  tensor\cf4 (\cf5 57.5774\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 9\cf4 ):\cf2  tensor\cf4 (\cf5 53.2205\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 10\cf4 ):\cf2  tensor\cf4 (\cf5 48.1375\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 11\cf4 ):\cf2  tensor\cf4 (\cf5 43.2684\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 12\cf4 ):\cf2  tensor\cf4 (\cf5 39.5132\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 13\cf4 ):\cf2  tensor\cf4 (\cf5 35.6761\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 14\cf4 ):\cf2  tensor\cf4 (\cf5 33.3417\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 15\cf4 ):\cf2  tensor\cf4 (\cf5 27.0982\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 16\cf4 ):\cf2  tensor\cf4 (\cf5 23.9728\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 17\cf4 ):\cf2  tensor\cf4 (\cf5 22.4406\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 18\cf4 ):\cf2  tensor\cf4 (\cf5 20.5941\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 19\cf4 ):\cf2  tensor\cf4 (\cf5 17.9789\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 20\cf4 ):\cf2  tensor\cf4 (\cf5 14.9816\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 21\cf4 ):\cf2  tensor\cf4 (\cf5 14.3930\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 22\cf4 ):\cf2  tensor\cf4 (\cf5 12.3972\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 23\cf4 ):\cf2  tensor\cf4 (\cf5 11.4945\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 24\cf4 ):\cf2  tensor\cf4 (\cf5 10.2228\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 25\cf4 ):\cf2  tensor\cf4 (\cf5 8.2635\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 26\cf4 ):\cf2  tensor\cf4 (\cf5 10.1588\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 27\cf4 ):\cf2  tensor\cf4 (\cf5 10.7962\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 28\cf4 ):\cf2  tensor\cf4 (\cf5 5.1332\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 29\cf4 ):\cf2  tensor\cf4 (\cf5 8.8454\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 30\cf4 ):\cf2  tensor\cf4 (\cf5 5.5052\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 31\cf4 ):\cf2  tensor\cf4 (\cf5 6.3038\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 32\cf4 ):\cf2  tensor\cf4 (\cf5 7.4683\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 33\cf4 ):\cf2  tensor\cf4 (\cf5 2.7190\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 34\cf4 ):\cf2  tensor\cf4 (\cf5 2.6773\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 35\cf4 ):\cf2  tensor\cf4 (\cf5 8.0594\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 36\cf4 ):\cf2  tensor\cf4 (\cf5 7.5492\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 37\cf4 ):\cf2  tensor\cf4 (\cf5 8.0731\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 38\cf4 ):\cf2  tensor\cf4 (\cf5 2.5849\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 39\cf4 ):\cf2  tensor\cf4 (\cf5 1.4765\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 40\cf4 ):\cf2  tensor\cf4 (\cf5 1.5589\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 0\cf4 ):\cf2  tensor\cf4 (\cf5 193.7610\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 1\cf4 ):\cf2  tensor\cf4 (\cf5 141.0777\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 2\cf4 ):\cf2  tensor\cf4 (\cf5 121.7593\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 3\cf4 ):\cf2  tensor\cf4 (\cf5 107.5621\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 4\cf4 ):\cf2  tensor\cf4 (\cf5 94.2178\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 5\cf4 ):\cf2  tensor\cf4 (\cf5 83.9467\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 6\cf4 ):\cf2  tensor\cf4 (\cf5 73.1026\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 7\cf4 ):\cf2  tensor\cf4 (\cf5 67.6756\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 8\cf4 ):\cf2  tensor\cf4 (\cf5 61.1910\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 9\cf4 ):\cf2  tensor\cf4 (\cf5 54.2516\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 10\cf4 ):\cf2  tensor\cf4 (\cf5 49.2229\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 11\cf4 ):\cf2  tensor\cf4 (\cf5 43.5535\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 12\cf4 ):\cf2  tensor\cf4 (\cf5 39.7998\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 13\cf4 ):\cf2  tensor\cf4 (\cf5 34.3079\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 14\cf4 ):\cf2  tensor\cf4 (\cf5 32.0837\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 15\cf4 ):\cf2  tensor\cf4 (\cf5 28.0917\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 16\cf4 ):\cf2  tensor\cf4 (\cf5 26.0610\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 17\cf4 ):\cf2  tensor\cf4 (\cf5 23.0750\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 18\cf4 ):\cf2  tensor\cf4 (\cf5 18.8138\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 19\cf4 ):\cf2  tensor\cf4 (\cf5 15.8798\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 20\cf4 ):\cf2  tensor\cf4 (\cf5 13.9187\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 21\cf4 ):\cf2  tensor\cf4 (\cf5 12.9794\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 22\cf4 ):\cf2  tensor\cf4 (\cf5 11.9072\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 23\cf4 ):\cf2  tensor\cf4 (\cf5 10.0925\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 24\cf4 ):\cf2  tensor\cf4 (\cf5 11.4917\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 25\cf4 ):\cf2  tensor\cf4 (\cf5 9.6616\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 26\cf4 ):\cf2  tensor\cf4 (\cf5 7.3927\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 27\cf4 ):\cf2  tensor\cf4 (\cf5 6.9416\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 28\cf4 ):\cf2  tensor\cf4 (\cf5 6.0785\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 29\cf4 ):\cf2  tensor\cf4 (\cf5 3.9905\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 30\cf4 ):\cf2  tensor\cf4 (\cf5 9.5330\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 31\cf4 ):\cf2  tensor\cf4 (\cf5 5.1096\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 32\cf4 ):\cf2  tensor\cf4 (\cf5 5.1695\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 33\cf4 ):\cf2  tensor\cf4 (\cf5 5.3122\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 34\cf4 ):\cf2  tensor\cf4 (\cf5 4.5847\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 35\cf4 ):\cf2  tensor\cf4 (\cf5 3.3690\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 36\cf4 ):\cf2  tensor\cf4 (\cf5 1.4171\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 37\cf4 ):\cf2  tensor\cf4 (\cf5 1.2238\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 38\cf4 ):\cf2  tensor\cf4 (\cf5 1.2540\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 39\cf4 ):\cf2  tensor\cf4 (\cf5 0.9807\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 40\cf4 ):\cf2  tensor\cf4 (\cf5 7.6732\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 0\cf4 ):\cf2  tensor\cf4 (\cf5 187.3310\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 1\cf4 ):\cf2  tensor\cf4 (\cf5 141.4971\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 2\cf4 ):\cf2  tensor\cf4 (\cf5 119.9875\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 3\cf4 ):\cf2  tensor\cf4 (\cf5 103.9088\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 4\cf4 ):\cf2  tensor\cf4 (\cf5 92.8178\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 5\cf4 ):\cf2  tensor\cf4 (\cf5 82.5932\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 6\cf4 ):\cf2  tensor\cf4 (\cf5 72.7286\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 7\cf4 ):\cf2  tensor\cf4 (\cf5 65.7953\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 8\cf4 ):\cf2  tensor\cf4 (\cf5 57.9509\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 9\cf4 ):\cf2  tensor\cf4 (\cf5 52.2875\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 10\cf4 ):\cf2  tensor\cf4 (\cf5 45.2451\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 11\cf4 ):\cf2  tensor\cf4 (\cf5 41.1260\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 12\cf4 ):\cf2  tensor\cf4 (\cf5 36.7670\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 13\cf4 ):\cf2  tensor\cf4 (\cf5 32.3427\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 14\cf4 ):\cf2  tensor\cf4 (\cf5 28.3757\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 15\cf4 ):\cf2  tensor\cf4 (\cf5 24.9938\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 16\cf4 ):\cf2  tensor\cf4 (\cf5 23.6090\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 17\cf4 ):\cf2  tensor\cf4 (\cf5 20.6633\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 18\cf4 ):\cf2  tensor\cf4 (\cf5 16.7310\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 19\cf4 ):\cf2  tensor\cf4 (\cf5 14.3486\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 20\cf4 ):\cf2  tensor\cf4 (\cf5 14.8130\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 21\cf4 ):\cf2  tensor\cf4 (\cf5 12.0431\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 22\cf4 ):\cf2  tensor\cf4 (\cf5 8.8802\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 23\cf4 ):\cf2  tensor\cf4 (\cf5 9.7282\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 24\cf4 ):\cf2  tensor\cf4 (\cf5 7.8661\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 25\cf4 ):\cf2  tensor\cf4 (\cf5 8.7723\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 26\cf4 ):\cf2  tensor\cf4 (\cf5 8.6279\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 27\cf4 ):\cf2  tensor\cf4 (\cf5 8.1194\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 28\cf4 ):\cf2  tensor\cf4 (\cf5 6.3047\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 29\cf4 ):\cf2  tensor\cf4 (\cf5 6.4378\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 30\cf4 ):\cf2  tensor\cf4 (\cf5 2.9178\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 31\cf4 ):\cf2  tensor\cf4 (\cf5 2.1987\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 32\cf4 ):\cf2  tensor\cf4 (\cf5 1.7045\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 33\cf4 ):\cf2  tensor\cf4 (\cf5 3.3757\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 34\cf4 ):\cf2  tensor\cf4 (\cf5 20.0250\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 35\cf4 ):\cf2  tensor\cf4 (\cf5 6.3262\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 36\cf4 ):\cf2  tensor\cf4 (\cf5 2.2830\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 37\cf4 ):\cf2  tensor\cf4 (\cf5 1.4365\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 38\cf4 ):\cf2  tensor\cf4 (\cf5 1.1589\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 39\cf4 ):\cf2  tensor\cf4 (\cf5 1.0235\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 40\cf4 ):\cf2  tensor\cf4 (\cf5 1.1344\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 0\cf4 ):\cf2  tensor\cf4 (\cf5 188.7731\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 1\cf4 ):\cf2  tensor\cf4 (\cf5 136.1839\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 2\cf4 ):\cf2  tensor\cf4 (\cf5 115.6736\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 3\cf4 ):\cf2  tensor\cf4 (\cf5 98.2454\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 4\cf4 ):\cf2  tensor\cf4 (\cf5 85.5077\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 5\cf4 ):\cf2  tensor\cf4 (\cf5 78.4982\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 6\cf4 ):\cf2  tensor\cf4 (\cf5 67.8870\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 7\cf4 ):\cf2  tensor\cf4 (\cf5 62.8143\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 8\cf4 ):\cf2  tensor\cf4 (\cf5 54.5082\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 9\cf4 ):\cf2  tensor\cf4 (\cf5 48.9467\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 10\cf4 ):\cf2  tensor\cf4 (\cf5 43.8368\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 11\cf4 ):\cf2  tensor\cf4 (\cf5 40.1112\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 12\cf4 ):\cf2  tensor\cf4 (\cf5 34.1633\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 13\cf4 ):\cf2  tensor\cf4 (\cf5 31.0446\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 14\cf4 ):\cf2  tensor\cf4 (\cf5 25.5887\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 15\cf4 ):\cf2  tensor\cf4 (\cf5 23.1227\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 16\cf4 ):\cf2  tensor\cf4 (\cf5 20.4912\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 17\cf4 ):\cf2  tensor\cf4 (\cf5 19.5932\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 18\cf4 ):\cf2  tensor\cf4 (\cf5 15.7111\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 19\cf4 ):\cf2  tensor\cf4 (\cf5 13.6985\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 20\cf4 ):\cf2  tensor\cf4 (\cf5 11.9244\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 21\cf4 ):\cf2  tensor\cf4 (\cf5 12.3332\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 22\cf4 ):\cf2  tensor\cf4 (\cf5 10.5092\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 23\cf4 ):\cf2  tensor\cf4 (\cf5 9.1739\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 24\cf4 ):\cf2  tensor\cf4 (\cf5 8.4241\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 25\cf4 ):\cf2  tensor\cf4 (\cf5 14.3089\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 26\cf4 ):\cf2  tensor\cf4 (\cf5 6.1834\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 27\cf4 ):\cf2  tensor\cf4 (\cf5 3.8661\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 28\cf4 ):\cf2  tensor\cf4 (\cf5 3.0073\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 29\cf4 ):\cf2  tensor\cf4 (\cf5 6.5293\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 30\cf4 ):\cf2  tensor\cf4 (\cf5 4.3720\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 31\cf4 ):\cf2  tensor\cf4 (\cf5 2.0786\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 32\cf4 ):\cf2  tensor\cf4 (\cf5 1.6460\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 33\cf4 ):\cf2  tensor\cf4 (\cf5 8.7446\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 34\cf4 ):\cf2  tensor\cf4 (\cf5 17.4896\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 35\cf4 ):\cf2  tensor\cf4 (\cf5 3.4635\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 36\cf4 ):\cf2  tensor\cf4 (\cf5 1.6633\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 37\cf4 ):\cf2  tensor\cf4 (\cf5 1.0972\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 38\cf4 ):\cf2  tensor\cf4 (\cf5 0.8687\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 39\cf4 ):\cf2  tensor\cf4 (\cf5 0.7724\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 40\cf4 ):\cf2  tensor\cf4 (\cf5 0.7880\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 0\cf4 ):\cf2  tensor\cf4 (\cf5 190.0059\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 1\cf4 ):\cf2  tensor\cf4 (\cf5 139.8280\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 2\cf4 ):\cf2  tensor\cf4 (\cf5 118.4801\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 3\cf4 ):\cf2  tensor\cf4 (\cf5 104.2947\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 4\cf4 ):\cf2  tensor\cf4 (\cf5 93.8550\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 5\cf4 ):\cf2  tensor\cf4 (\cf5 83.5021\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 6\cf4 ):\cf2  tensor\cf4 (\cf5 73.7931\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 7\cf4 ):\cf2  tensor\cf4 (\cf5 68.1245\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 8\cf4 ):\cf2  tensor\cf4 (\cf5 59.6167\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 9\cf4 ):\cf2  tensor\cf4 (\cf5 54.9305\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 10\cf4 ):\cf2  tensor\cf4 (\cf5 48.9651\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 11\cf4 ):\cf2  tensor\cf4 (\cf5 43.4238\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 12\cf4 ):\cf2  tensor\cf4 (\cf5 39.6463\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 13\cf4 ):\cf2  tensor\cf4 (\cf5 34.7221\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 14\cf4 ):\cf2  tensor\cf4 (\cf5 31.2384\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 15\cf4 ):\cf2  tensor\cf4 (\cf5 28.0169\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 16\cf4 ):\cf2  tensor\cf4 (\cf5 24.7731\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 17\cf4 ):\cf2  tensor\cf4 (\cf5 21.2537\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 18\cf4 ):\cf2  tensor\cf4 (\cf5 18.9696\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 19\cf4 ):\cf2  tensor\cf4 (\cf5 16.8214\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 20\cf4 ):\cf2  tensor\cf4 (\cf5 14.3991\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 21\cf4 ):\cf2  tensor\cf4 (\cf5 14.4506\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 22\cf4 ):\cf2  tensor\cf4 (\cf5 11.2810\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 23\cf4 ):\cf2  tensor\cf4 (\cf5 12.3428\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 24\cf4 ):\cf2  tensor\cf4 (\cf5 9.4737\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 25\cf4 ):\cf2  tensor\cf4 (\cf5 7.3255\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 26\cf4 ):\cf2  tensor\cf4 (\cf5 7.0013\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 27\cf4 ):\cf2  tensor\cf4 (\cf5 5.8803\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 28\cf4 ):\cf2  tensor\cf4 (\cf5 15.4891\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 29\cf4 ):\cf2  tensor\cf4 (\cf5 11.8453\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 30\cf4 ):\cf2  tensor\cf4 (\cf5 4.2254\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 31\cf4 ):\cf2  tensor\cf4 (\cf5 4.7264\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 32\cf4 ):\cf2  tensor\cf4 (\cf5 3.4039\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 33\cf4 ):\cf2  tensor\cf4 (\cf5 2.4454\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 34\cf4 ):\cf2  tensor\cf4 (\cf5 2.2133\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 35\cf4 ):\cf2  tensor\cf4 (\cf5 6.8356\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 36\cf4 ):\cf2  tensor\cf4 (\cf5 21.5518\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 37\cf4 ):\cf2  tensor\cf4 (\cf5 7.1963\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 38\cf4 ):\cf2  tensor\cf4 (\cf5 2.2615\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 39\cf4 ):\cf2  tensor\cf4 (\cf5 1.7130\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 40\cf4 ):\cf2  tensor\cf4 (\cf5 1.5228\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 0\cf4 ):\cf2  tensor\cf4 (\cf5 188.3868\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 1\cf4 ):\cf2  tensor\cf4 (\cf5 139.1391\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 2\cf4 ):\cf2  tensor\cf4 (\cf5 117.4462\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 3\cf4 ):\cf2  tensor\cf4 (\cf5 100.7780\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 4\cf4 ):\cf2  tensor\cf4 (\cf5 90.0881\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 5\cf4 ):\cf2  tensor\cf4 (\cf5 78.9769\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 6\cf4 ):\cf2  tensor\cf4 (\cf5 70.6340\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 7\cf4 ):\cf2  tensor\cf4 (\cf5 63.6427\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 8\cf4 ):\cf2  tensor\cf4 (\cf5 56.9904\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 9\cf4 ):\cf2  tensor\cf4 (\cf5 50.8272\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 10\cf4 ):\cf2  tensor\cf4 (\cf5 45.6536\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 11\cf4 ):\cf2  tensor\cf4 (\cf5 41.3692\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 12\cf4 ):\cf2  tensor\cf4 (\cf5 35.8880\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 13\cf4 ):\cf2  tensor\cf4 (\cf5 32.6587\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 14\cf4 ):\cf2  tensor\cf4 (\cf5 29.0403\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 15\cf4 ):\cf2  tensor\cf4 (\cf5 26.5610\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 16\cf4 ):\cf2  tensor\cf4 (\cf5 22.1615\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 17\cf4 ):\cf2  tensor\cf4 (\cf5 19.9196\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 18\cf4 ):\cf2  tensor\cf4 (\cf5 16.7064\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 19\cf4 ):\cf2  tensor\cf4 (\cf5 16.7899\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 20\cf4 ):\cf2  tensor\cf4 (\cf5 15.0233\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 21\cf4 ):\cf2  tensor\cf4 (\cf5 11.2389\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 22\cf4 ):\cf2  tensor\cf4 (\cf5 10.3776\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 23\cf4 ):\cf2  tensor\cf4 (\cf5 13.6122\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 24\cf4 ):\cf2  tensor\cf4 (\cf5 10.1891\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 25\cf4 ):\cf2  tensor\cf4 (\cf5 7.4283\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 26\cf4 ):\cf2  tensor\cf4 (\cf5 4.8029\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 27\cf4 ):\cf2  tensor\cf4 (\cf5 7.2127\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 28\cf4 ):\cf2  tensor\cf4 (\cf5 6.8847\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 29\cf4 ):\cf2  tensor\cf4 (\cf5 7.0359\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 30\cf4 ):\cf2  tensor\cf4 (\cf5 11.9120\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 31\cf4 ):\cf2  tensor\cf4 (\cf5 5.1244\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 32\cf4 ):\cf2  tensor\cf4 (\cf5 3.1478\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 33\cf4 ):\cf2  tensor\cf4 (\cf5 7.1279\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 34\cf4 ):\cf2  tensor\cf4 (\cf5 4.0687\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 35\cf4 ):\cf2  tensor\cf4 (\cf5 1.9356\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 36\cf4 ):\cf2  tensor\cf4 (\cf5 3.7113\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 37\cf4 ):\cf2  tensor\cf4 (\cf5 14.5057\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 38\cf4 ):\cf2  tensor\cf4 (\cf5 7.1088\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 39\cf4 ):\cf2  tensor\cf4 (\cf5 2.3583\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 40\cf4 ):\cf2  tensor\cf4 (\cf5 1.3976\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 0\cf4 ):\cf2  tensor\cf4 (\cf5 192.5982\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 1\cf4 ):\cf2  tensor\cf4 (\cf5 138.8615\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 2\cf4 ):\cf2  tensor\cf4 (\cf5 114.4880\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 3\cf4 ):\cf2  tensor\cf4 (\cf5 97.4043\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 4\cf4 ):\cf2  tensor\cf4 (\cf5 85.5786\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 5\cf4 ):\cf2  tensor\cf4 (\cf5 76.5718\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 6\cf4 ):\cf2  tensor\cf4 (\cf5 68.0295\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 7\cf4 ):\cf2  tensor\cf4 (\cf5 59.1599\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 8\cf4 ):\cf2  tensor\cf4 (\cf5 53.3422\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 9\cf4 ):\cf2  tensor\cf4 (\cf5 47.9872\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 10\cf4 ):\cf2  tensor\cf4 (\cf5 41.7606\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 11\cf4 ):\cf2  tensor\cf4 (\cf5 38.6640\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 12\cf4 ):\cf2  tensor\cf4 (\cf5 33.4735\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 13\cf4 ):\cf2  tensor\cf4 (\cf5 28.3458\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 14\cf4 ):\cf2  tensor\cf4 (\cf5 26.0397\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 15\cf4 ):\cf2  tensor\cf4 (\cf5 21.6482\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 16\cf4 ):\cf2  tensor\cf4 (\cf5 19.7597\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 17\cf4 ):\cf2  tensor\cf4 (\cf5 16.3161\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 18\cf4 ):\cf2  tensor\cf4 (\cf5 14.8924\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 19\cf4 ):\cf2  tensor\cf4 (\cf5 12.2909\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 20\cf4 ):\cf2  tensor\cf4 (\cf5 10.3358\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 21\cf4 ):\cf2  tensor\cf4 (\cf5 13.2843\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 22\cf4 ):\cf2  tensor\cf4 (\cf5 8.6275\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 23\cf4 ):\cf2  tensor\cf4 (\cf5 7.0723\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 24\cf4 ):\cf2  tensor\cf4 (\cf5 7.2705\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 25\cf4 ):\cf2  tensor\cf4 (\cf5 4.5372\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 26\cf4 ):\cf2  tensor\cf4 (\cf5 5.3164\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 27\cf4 ):\cf2  tensor\cf4 (\cf5 16.6559\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 28\cf4 ):\cf2  tensor\cf4 (\cf5 4.2655\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 29\cf4 ):\cf2  tensor\cf4 (\cf5 2.3589\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 30\cf4 ):\cf2  tensor\cf4 (\cf5 2.1362\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 31\cf4 ):\cf2  tensor\cf4 (\cf5 6.4966\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 32\cf4 ):\cf2  tensor\cf4 (\cf5 12.5309\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 33\cf4 ):\cf2  tensor\cf4 (\cf5 4.6414\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 34\cf4 ):\cf2  tensor\cf4 (\cf5 1.8567\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 35\cf4 ):\cf2  tensor\cf4 (\cf5 1.3681\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 36\cf4 ):\cf2  tensor\cf4 (\cf5 1.2646\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 37\cf4 ):\cf2  tensor\cf4 (\cf5 1.2008\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 38\cf4 ):\cf2  tensor\cf4 (\cf5 1.1195\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 39\cf4 ):\cf2  tensor\cf4 (\cf5 0.9066\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
\cb3         Training Loss \cf4 (\cf2 after epoch \cf5 40\cf4 ):\cf2  tensor\cf4 (\cf5 0.8265\cf4 ,\cf2  grad_fn=<AddBackward0>\cf4 )\cf2 \cb1 \
}